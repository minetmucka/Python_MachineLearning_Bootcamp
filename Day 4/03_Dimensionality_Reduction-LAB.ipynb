{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Agenda\n",
    "+ Exploratory Data Analysis\n",
    "+ Scale data appropriately\n",
    "+ Use sklearn's pipeline\n",
    "+ Apply and evaluate PCA and Kernel PCA\n",
    "+ _Optional_ Use PCA in a pipeline with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    FunctionTransformer,\n",
    "    StandardScaler\n",
    ")\n",
    "\n",
    "from sklearn.decomposition import (\n",
    "    PCA,\n",
    "    KernelPCA\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedShuffleSplit,\n",
    "    GridSearchCV\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    mean_squared_error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using customer data from a [Portuguese wholesale distributor](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers) for clustering. This data file is called `Wholesale_Customers_Data`.\n",
    "\n",
    "It contains the following features:\n",
    "\n",
    "* Fresh: annual spending (m.u.) on fresh products\n",
    "* Milk: annual spending (m.u.) on milk products\n",
    "* Grocery: annual spending (m.u.) on grocery products\n",
    "* Frozen: annual spending (m.u.) on frozen products\n",
    "* Detergents_Paper: annual spending (m.u.) on detergents and paper products\n",
    "* Delicatessen: annual spending (m.u.) on delicatessen products\n",
    "* Channel: customer channel (1: hotel/restaurant/cafe or 2: retail)\n",
    "* Region: customer region (1: Lisbon, 2: Porto, 3: Other)\n",
    "\n",
    "In this data, the values for all spending are given in an arbitrary unit (m.u. = monetary unit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "* Import the data and check the data types.\n",
    "* Drop the channel and region columns as they won't be used.\n",
    "* Convert the remaining columns to floats if necessary.\n",
    "* Copy this version of the data (using the `copy` method) to a variable to preserve it. We will be using it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = pathlib.Path.cwd() / 'Resources' /  'Wholesale_Customers_Data.csv'\n",
    "data = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "As with the previous labs, we need to ensure the data is scaled and (relatively) normally distributed.\n",
    "\n",
    "* Examine the correlation and skew.\n",
    "* Perform any transformations and scale data using your favorite scaling method.\n",
    "* View the pairwise correlation plots of the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Using Scikit-learn's [pipeline function](http://scikit-learn.org/stable/modules/pipeline.html), recreate the data pre-processing scheme above (transformation and scaling) using a pipeline. If you used a non-Scikit learn function to transform the data (e.g. NumPy's log function), checkout  the custom transformer class called [`FunctionTransformer`](http://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers).\n",
    "* Use the pipeline to transform the original data that was stored at the end of question 1.\n",
    "* Compare the results to the original data to verify that everything worked.\n",
    "\n",
    "*Hint:* Scikit-learn has a more flexible `Pipeline` function and a shortcut version called `make_pipeline`. Either can be used. Also, if different transformations need to be performed on the data, a [`FeatureUnion`](http://scikit-learn.org/stable/modules/pipeline.html#featureunion-composite-feature-spaces) can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "* Perform PCA with `n_components` ranging from 1 to 5. \n",
    "* Store the amount of explained variance for each number of dimensions.\n",
    "* Also store the feature importance for each number of dimensions. *Hint:* PCA doesn't explicitly provide this after a model is fit, but the `components_` properties can be used to determine something that approximates importance. How you decided to do so is entirely up to you.\n",
    "* Plot the explained variance and feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "* Fit a `KernelPCA` model with `kernel='rbf'`. You can choose how many components and what values to use for the other parameters.\n",
    "* If you want to tinker some more, use `GridSearchCV` to tune the parameters of the `KernelPCA` model. \n",
    "\n",
    "The second step is tricky since grid searches are generally used for supervised machine learning methods and rely on scoring metrics, such as accuracy, to determine the best model. However, a custom scoring function can be written for `GridSearchCV`, where larger is better for the outcome of the scoring function. \n",
    "\n",
    "What would such a metric involve for PCA? What about percent of explained variance? Or perhaps the negative mean squared error on the data once it has been transformed and then inversely transformed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [_Optional_] Question 6\n",
    "\n",
    "Let's explore how our model accuracy may change if we include a `PCA` in our model building pipeline. Let's plan to use sklearn's `Pipeline` class and create a pipeline that has the following steps:\n",
    "<ol>\n",
    "  <li>A scaler</li>\n",
    "  <li>`PCA(n_components=n)`</li>\n",
    "  <li>`LogisticRegression`</li>\n",
    "</ol>\n",
    "\n",
    "* Load the Human Activity data from the datasets.\n",
    "* Write a function that takes in a value of `n` and makes the above pipeline, then predicts the \"Activity\" column over a 5-fold StratifiedShuffleSplit, and returns the average test accuracy\n",
    "* For various values of n, call the above function and store the average accuracies.\n",
    "* Plot the average accuracy by number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = pathlib.Path.cwd() / 'Resources' /  'Human_Activity_Recognition_Using_Smartphones_Data.csv'\n",
    "data2 = pd.read_csv(filepath, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
