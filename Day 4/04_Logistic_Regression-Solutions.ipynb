{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression Agenda\n",
    "\n",
    "  * Attempt to use linear regression for classification\n",
    "  * Logistic regression is a better alternative for classification\n",
    "  * Brief overview of probability, odds, e, log, and log-odds\n",
    "  * What is the logistic regression model?\n",
    "  * Interpreting logistic regression coefficients\n",
    "  * Compare logistic regression with other models\n",
    "  \n",
    "By the end of this portion of the class you will be able to:\n",
    "  * Use logistic regression for a classification problem in the future\n",
    "  * interpret the coefficients of a trained logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting a categorical response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the first part of today's lesson, we were attempting to predict a **continuous response**. However, what we want to do now is see if we can apply the same sort of logic to predict an outcome that has only 2 distinct possibilities, or what is known as a **categorical response.**\n",
    "\n",
    "In machine learning parlance, we looked at **regression** when we were using linear regression, but we are now going to try to use the same approach for what is known as a **classification** problem (problems with only a discrete, finite number of outcomes; in our case, just 2).\n",
    "\n",
    "As always, we are going to import all of the functionality we need before we get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#data handling/modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import scipy.stats as stats\n",
    "\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 6 biomechanical features used to classify orthopaedic patients into 2 classes - normal and abnormal:\n",
    "  * pelvic incidence\n",
    "  * pelvic tilt\n",
    "  * lumbar lordosis angle\n",
    "  * sacral slope\n",
    "  * pelvic radius\n",
    "  * grade of spondylolisthesis\n",
    "  \n",
    "Lets load the data in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertebral_data = pd.read_csv(\"https://s3-us-west-1.amazonaws.com/linkedin-metis/vertebral_train.csv\")\n",
    "vertebral_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, this wouldn't be a data science class without a CSV that was tricky to read in. Try using the following to look at a \"raw text\" version of the file:\n",
    "\n",
    "```python\n",
    "import requests as r\n",
    "resp = r.get(filename)\n",
    "print(resp.text)\n",
    "```\n",
    "\n",
    "Then, read the Pandas [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) documentation to figure out how to read in this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import requests as r\n",
    "resp = r.get(\"https://s3-us-west-1.amazonaws.com/linkedin-metis/vertebral_train.csv\")\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertebral_data = pd.read_csv(\"https://s3-us-west-1.amazonaws.com/linkedin-metis/vertebral_train.csv\", sep=\" \")\n",
    "vertebral_data.outcome.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use linear regression for this task, we have to convert our **categorical** target into a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertebral_data[\"outcome_number\"] = (vertebral_data.outcome=='AB').astype(int)\n",
    "vertebral_data.outcome_number.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so now our outcome is no longer a value, but a number. Let's plot `pelvic_incidence` relative to this new numeric `outcome_number`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(vertebral_data,x_vars=[\"pelvic_incidence\"],y_vars=\"outcome_number\", size=6, aspect=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And now lets do a simple linear regression on that feature like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# fit a linear regression model and store the predictions\n",
    "feature_cols = ['pelvic_incidence']\n",
    "X = vertebral_data[feature_cols]\n",
    "y = vertebral_data.outcome_number\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "outcome_pred = linreg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Here, we are just trying to determine what the best cutoff for classifying the vertebral data as class 0 or 1, not the accuracy of such a model, so we want to use all available data rather than splitting into a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# scatter plot that includes the regression line\n",
    "plt.scatter(vertebral_data.pelvic_incidence, vertebral_data.outcome_number)\n",
    "plt.plot(vertebral_data.pelvic_incidence, outcome_pred, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets examine the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If **pelvic_incidence=35**, what class do we predict for outcome? **0**\n",
    "\n",
    "So, we predict the 0 class for **lower** values of `pelvic_incidence`, and the 1 class for **higher** values of `pelvic_incidence`. What's our cutoff value? Around **pelvic_incidence=45**, because that's where the linear regression line crosses the midpoint (0.5) between predicting class 0 and class 1.\n",
    "\n",
    "So, we'll say that if **outcome_pred >= 0.5**, we predict a class of **1**, else we predict a class of **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# np.where returns the first value if the condition is True, and the second value if the condition is False\n",
    "np.where(outcome_pred >= .5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# transform predictions to 1 or 0\n",
    "outcome_pred_class = np.where(outcome_pred >= 0.5, 1, 0)\n",
    "outcome_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the class predictions\n",
    "plt.scatter(vertebral_data.pelvic_incidence, vertebral_data.outcome_number)\n",
    "plt.plot(vertebral_data.pelvic_incidence, outcome_pred_class, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What went wrong? This is a line plot, and it connects points in the order they are found. Let's sort the DataFrame by \"al\" to fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# add predicted class to DataFrame\n",
    "vertebral_data['outcome_pred_class'] = outcome_pred_class\n",
    "\n",
    "# sort DataFrame by pelvic_incidence so that the line plot makes sense\n",
    "vertebral_data.sort_values('pelvic_incidence', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the class predictions\n",
    "plt.scatter(vertebral_data.pelvic_incidence, vertebral_data.outcome_number)\n",
    "plt.plot(vertebral_data.pelvic_incidence,\n",
    "         outcome_pred_class, \n",
    "         color='red'\n",
    "        );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(vertebral_data.outcome_number == vertebral_data.outcome_pred_class).sum()/vertebral_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use Logistic Regression Instead of Linear Regression on Categorical Outcome Variables\n",
    "\n",
    "Logistic regression can do exactly what we just did:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1e9)\n",
    "feature_cols = ['pelvic_incidence']\n",
    "X = vertebral_data[feature_cols]\n",
    "y = vertebral_data.outcome_number\n",
    "logreg.fit(X, y)\n",
    "outcome_pred_class_log = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print the class predictions\n",
    "outcome_pred_class_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the class predictions\n",
    "plt.scatter(vertebral_data.pelvic_incidence, vertebral_data.outcome_number)\n",
    "plt.plot(vertebral_data.pelvic_incidence, outcome_pred_class_log, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if we wanted the **predicted probabilities** instead of just the **class predictions**, to understand how confident we are in a given prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# store the predicted probabilites of class 1\n",
    "outcome_probs = logreg.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plot the predicted probabilities, and the 50% line\n",
    "plt.scatter(vertebral_data.pelvic_incidence, vertebral_data.outcome_number)\n",
    "plt.plot(vertebral_data.pelvic_incidence, outcome_probs, color='red')\n",
    "plt.plot(vertebral_data.pelvic_incidence,np.ones(outcome_probs.shape)*.5,'k--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# examine some example predictions\n",
    "print(\"Pelvic incidence of 15:\", logreg.predict_proba([[15]]))\n",
    "print(\"Pelvic incidence of 10:\", logreg.predict_proba([[10]]))\n",
    "print(\"Pelvic incidence of 55:\", logreg.predict_proba([[55]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What are these numbers? \n",
    "\n",
    "The first number in each entry indicates the predicted probability of **class 0**, and the second number in each entry indicates the predicted probability of **class 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review: Probability, odds, e, log, log-odds\n",
    "\n",
    "$$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n",
    "\n",
    "$$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Dice roll of 1: probability = 1/6, odds = 1/5\n",
    "- Even dice roll: probability = 3/6, odds = 3/3 = 1\n",
    "- Dice roll less than 5: probability = 4/6, odds = 4/2 = 2\n",
    "\n",
    "$$odds = \\frac {probability} {1 - probability}$$\n",
    "\n",
    "$$probability = \\frac {odds} {1 + odds}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create a table of probability versus odds\n",
    "prob_table = pd.DataFrame({'probability':[0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99, 0.999]})\n",
    "prob_table['odds'] = prob_table.probability/(1 - prob_table.probability)\n",
    "prob_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is **e**? It is the base rate of growth shared by all continually growing processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# exponential function: e^1\n",
    "np.exp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is a **(natural) log**? It gives you the time needed to reach a certain level of growth ([wiki](https://en.wikipedia.org/wiki/Natural_logarithm)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# time needed to grow 1 unit to 2.718 units\n",
    "np.log(np.exp(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is also the **inverse** of the exponential function ([review your properties of logarithms here](http://www.purplemath.com/modules/logrules.htm)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.log(np.exp(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# add log-odds to the table\n",
    "prob_table['log_odds'] = np.log(prob_table.odds)\n",
    "prob_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ok, but what is logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[**Linear regression:**](https://en.wikipedia.org/wiki/Linear_regression) continuous response is modeled as a linear combination of the features used :\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x + ... \\beta_nx$$\n",
    "\n",
    "[**Logistic regression:**](https://en.wikipedia.org/wiki/Logistic_regression) log-odds of a categorical response being \"true\" (or the number 1) is modeled as a linear combination of the features:\n",
    "\n",
    "$$\\log \\left({p\\over 1-p}\\right) = \\beta_0 + \\beta_1x + ... \\beta_nx$$\n",
    "\n",
    "This is called the [**logit function**](https://en.wikipedia.org/wiki/Logit).\n",
    "\n",
    "Probability is sometimes written as pi:\n",
    "\n",
    "$$\\log \\left({\\pi\\over 1-\\pi}\\right) = \\beta_0 + \\beta_1x + ... \\beta_nx$$\n",
    "\n",
    "The equation can be rearranged into the [**logistic function**](https://en.wikipedia.org/wiki/Logistic_function):\n",
    "\n",
    "$$\\pi = \\frac{e^{\\beta_0 + \\beta_1x + ... + \\beta_nx}} {1 + e^{\\beta_0 + \\beta_1x + ... + \\beta_nx}}$$\n",
    "\n",
    "Here's what that looks like:\n",
    "\n",
    "![logistic curve](./images/logistic_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words:\n",
    "\n",
    "- Logistic regression outputs the **probabilities of a specific class**\n",
    "- Those probabilities can be converted into **class predictions**:\n",
    "\n",
    "$f(x)= \n",
    "\\begin{cases}\n",
    "    1,& \\text{if } p\\geq 0.5\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "The **logistic function** has some nice properties:\n",
    "\n",
    "- Takes on an \"s\" shape (which allows it to be differentiable, a really important math property for functions to have)\n",
    "- Output is bounded by 0 and 1\n",
    "\n",
    "Some things to note:\n",
    "\n",
    "- **Multinomial logistic regression** is used when there are more than 2 classes.\n",
    "- Coefficients are estimated using **maximum likelihood estimation**, meaning that we choose parameters that maximize the likelihood of the observed data. We do this using fancy math involving taking derivatives, and thats why that S-shaped curve is so important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting Logistic Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the predicted probabilities again\n",
    "plt.scatter(vertebral_data.pelvic_incidence, vertebral_data.outcome_number)\n",
    "plt.plot(vertebral_data.pelvic_incidence, outcome_probs, color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# compute predicted log-odds for pelvic_incidence=55 using the equation\n",
    "logodds = logreg.intercept_ + logreg.coef_[0] * 55\n",
    "print(\"Log odds:\",logodds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# convert log-odds to odds\n",
    "odds = np.exp(logodds)\n",
    "print(\"odds:\",odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# convert odds to probability, this is the number you would see in the plot above where x= 55\n",
    "prob = odds/(1 + odds)\n",
    "print(\"probability:\",prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# compute predicted probability for al=2 using the predict_proba method\n",
    "logreg.predict_proba([[55]])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# examine the coefficient for al\n",
    "dict(zip(feature_cols, logreg.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Interpretation:** A 1 unit increase in `pelvic_incidence` is associated with a ~0.046 unit increase in the log-odds of `outcome`, where a positive outcome is having a vertebral abnormality (not positive in the real world, but positive in how we coded our outcome feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# increasing pelvic_incidence by 1 (so that pelvic_incidence=56) increases the log-odds by about 0.046\n",
    "logodds = 0.57672042 + 0.04602586491752126\n",
    "odds = np.exp(logodds)\n",
    "prob = odds/(1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# compute predicted probability for al=3 using the predict_proba method\n",
    "logreg.predict_proba([[56]])[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What does this mean actually? \n",
    "\n",
    "**Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# examine the intercept\n",
    "logreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Interpretation:** For a 'pelvic_incidence' value of 0, the log-odds of 'outcome' is -1.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# convert log-odds to probability\n",
    "logodds = logreg.intercept_\n",
    "odds = np.exp(logodds)\n",
    "prob = odds/(1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That makes sense from the plot above, because the probability of outcome=1 should be very low for such a low `pelvic_incidence` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![logistic betas example](./images/logistic_betas_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we measure model performance for classification problems?\n",
    "\n",
    "Now that we have a trained model just as we did before with linear regression, what is our **evaluation metric/loss function**?\n",
    "\n",
    "There are two common (inverse) measurements we can make that capture the performance of our classification model:\n",
    "  * **Classification accuracy**: percentage of correct predictions (**reward function**)\n",
    "  * **Classification error**: percentage of incorrect predictions (**loss function**)\n",
    "\n",
    "In our case, we are going to use classification accuracy. Let's compute our classification accuracy after training on the whole dataset, using our just-trained one-feature model and the scikit-learn method `accuracy_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = vertebral_data.outcome_number\n",
    "y_pred = outcome_pred_class\n",
    "print(\"Model accuracy:\",metrics.accuracy_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "61% is ok, but its not really fantastic. Can we do better? (YES WE CAN!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "  * Generate the logistic regression model incorporating all of the features we have available to predict `outcome_number` and get the accuracy when training and testing on all data. How much better is this than the case where we trained our model using only `pelvic_incidence`?\n",
    "  * Use train/test split with 70% training, 30% testing and get the test error of the model trained on all features using `train_test_split` like we did during linear regression \n",
    "  * Inspect all of the model coefficients of the model trained on all features. Which feature is the most important for the prediction? Which is the least important?\n",
    "  * What are some problems you can see in using the data like we have been? (Look at the fraction of positive and negative outcomes in the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split concept for classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification data problems we want to used \"stratified\" train-test split: ensuring that the proportion of observations across the classes is even across the training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertebral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_col = \"outcome_number\"\n",
    "X = vertebral_data.drop([y_col,'outcome','outcome_pred_class'], axis=1)\n",
    "y = vertebral_data[y_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the out ome of the following two train-test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state=72018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    random_state=72018, \n",
    "                                                    stratify=y)\n",
    "\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructor Solutions\n",
    "\n",
    "\n",
    "logr = LogisticRegression()\n",
    "\n",
    "logr.fit(X_train,y_train)\n",
    "pd.DataFrame(zip(X_train.columns.values,logr.coef_[0])).sort_values(by=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and accuracy\n",
    "\n",
    "y_preds = logr.predict(X_test)\n",
    "print(\"new accuracy\",metrics.accuracy_score(y_test,y_preds))\n",
    "\n",
    "# Overall percentage of classes\n",
    "\n",
    "print(vertebral_data.outcome_number.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see by breakdown that by just predicting 1 every time we can already get 81% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing Logistic Regression with Other Models\n",
    "\n",
    "Logistic regression has some really awesome advantages:\n",
    "\n",
    "  * It is a highly interpretable method (if you remember what the conversions from log-odds to probability are)\n",
    "  * Model training and prediction are fast\n",
    "  * No tuning is required (excluding regularization, which we will talk about later)\n",
    "  * No need to scale features\n",
    "  * Outputs well-calibrated predicted probabilities (the probabilities behave like probabilities)\n",
    "\n",
    "However, logistic regression also has some disadvantages:\n",
    "\n",
    "  * It presumes a linear relationship between the features and the log-odds of the response\n",
    "  * Compared to other, more fancypants modeling approaches, performance is (generally) not competitive with the best supervised learning methods\n",
    "  * Like linear regression for regression, it is sensitive to irrelevant features\n",
    "  * Unless you explicitly code them (we will see how to do that later), logistic regression can't automatically learn feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Changing the $\\beta_0$ value shifts the curve **horizontally**, whereas changing the $\\beta_1$ value changes the **slope** of the curve.\n",
    "\n",
    "The non-bias $\\beta$ coefficients are effectively estimates of how certain you are of the outcome given how much evidence that specific feature gives you. A really high magnitude (positive or negative) value means you are very certain of the outcome, given you know that feature's value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (rest of day)\n",
    "\n",
    "**All the same tricks we learned with linear regression work with logistic regression as well!**\n",
    "\n",
    "* Standardizing your features produces more comparable coefficients\n",
    "* Regularization is implemented the same way and has the same interpretation: more regularization reduces model complexity/capacity and vice versa.\n",
    "\n",
    "With that said: \n",
    "\n",
    "**Try to build the best logistic regression model you can on the vertebral data**. I encourage you to use the cross validation functions from the prior lesson to compare models.\n",
    "\n",
    "I'll give you the testing data roughly 15 minutes before the end of class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
